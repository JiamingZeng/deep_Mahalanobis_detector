{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=200, dataset='cifar10', dataroot='./data', outf='output/', num_classes=10, net_type='resnet', gpu=0)\n",
      "load model: resnet\n",
      "load target data:  cifar10\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10-data/cifar-10-python.tar.gz\n",
      "100%|██████████████████████| 170498071/170498071 [00:01<00:00, 100075350.48it/s]\n",
      "Extracting ./data/cifar10-data/cifar-10-python.tar.gz to ./data/cifar10-data\n",
      "Files already downloaded and verified\n",
      "get sample mean and covariance\n",
      "/home/zjiaming/deep_Mahalanobis_detector/lib_generation.py:69: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data = Variable(data, volatile=True)\n",
      "\n",
      " Training Accuracy:(99.75%)\n",
      "\n",
      "get Mahalanobis scores\n",
      "Noise: 0.0\n",
      "/home/zjiaming/deep_Mahalanobis_detector/lib_generation.py:179: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Tensor input, Number alpha, Tensor other, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor input, Tensor other, *, Number alpha, Tensor out) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
      "  tempInputs = torch.add(data.data, -magnitude, gradient)\n",
      "/home/zjiaming/deep_Mahalanobis_detector/lib_generation.py:181: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  noise_out_features = model.intermediate_forward(Variable(tempInputs, volatile=True), layer_index)\n",
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/svhn-data/train_32x32.mat\n",
      "100%|███████████████████████| 182040794/182040794 [00:05<00:00, 32226426.14it/s]\n",
      "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/svhn-data/test_32x32.mat\n",
      "100%|█████████████████████████| 64275384/64275384 [00:04<00:00, 14577421.31it/s]\n",
      "Out-distribution: svhn\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zjiaming/deep_Mahalanobis_detector/OOD_Generate_Mahalanobis.py\", line 112, in <module>\n",
      "    main()\n",
      "  File \"/home/zjiaming/deep_Mahalanobis_detector/OOD_Generate_Mahalanobis.py\", line 93, in main\n",
      "    out_test_loader = data_loader.getNonTargetDataSet(out_dist, args.batch_size, in_transform, args.dataroot)\n",
      "  File \"/home/zjiaming/deep_Mahalanobis_detector/data_loader.py\", line 103, in getNonTargetDataSet\n",
      "    testsetout = datasets.ImageFolder(dataroot, transform=input_TF)\n",
      "  File \"/home/zjiaming/anaconda3/envs/randomTexture/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 310, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/zjiaming/anaconda3/envs/randomTexture/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 145, in __init__\n",
      "    classes, class_to_idx = self.find_classes(self.root)\n",
      "  File \"/home/zjiaming/anaconda3/envs/randomTexture/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 219, in find_classes\n",
      "    return find_classes(directory)\n",
      "  File \"/home/zjiaming/anaconda3/envs/randomTexture/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 41, in find_classes\n",
      "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './data/Imagenet_resize'\n"
     ]
    }
   ],
   "source": [
    "!python OOD_Generate_Mahalanobis.py --dataset cifar10 --outf output/ --net_type resnet --gpu 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(net_type='resnet')\n",
      "In-distribution:  cifar10\n",
      "Out-of-distribution:  svhn\n",
      "(36032, 5) (36032,)\n",
      "in_distribution: cifar10==========\n",
      "out_distribution: svhn\n",
      " TNR    AUROC  DTACC  AUIN   AUOUT \n",
      " 91.53  98.40  93.64  96.46  99.37\n",
      "Input noise: Mahalanobis_0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python OOD_Regression_Mahalanobis.py --net_type resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "     def __init__(self, input_dim, output_dim):\n",
    "         super(LogisticRegression, self).__init__()\n",
    "         self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "         \n",
    "     def forward(self, x):\n",
    "         outputs = torch.sigmoid(self.linear(x))\n",
    "         return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ConfidenceDataset(Dataset):\n",
    "    def __init__(self, confidence, labels):\n",
    "        self.labels = labels\n",
    "        self.confidence = confidence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.confidence)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        conf = self.confidence[idx]\n",
    "        sample = {\"Confidence\": conf, \"Class\": label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lib_regression\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "epochs = 5000\n",
    "input_dim = 5 # Two inputs x1 and x2 \n",
    "output_dim = 1 # Single binary output \n",
    "learning_rate = 0.000001\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "score = 'Mahalanobis_0.0'\n",
    "dataset = 'cifar10'\n",
    "out = 'svhn'\n",
    "outf = 'output/resnet_cifar10/'\n",
    "total_X, total_Y = lib_regression.load_characteristics(score, dataset, out, outf)\n",
    "X_val, Y_val, X_test, Y_test = lib_regression.block_split(total_X, total_Y, out)\n",
    "X_train = torch.tensor(np.concatenate((X_val[:500], X_val[1000:1500])), dtype=torch.float)\n",
    "\n",
    "Y_train = torch.tensor(np.concatenate((Y_val[:500], Y_val[1000:1500])), dtype=torch.float)\n",
    "train_dataset = ConfidenceDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,\n",
    "                                                shuffle=True, num_workers=1)\n",
    "X_val_for_test = torch.tensor(np.concatenate((X_val[500:1000], X_val[1500:])), dtype=torch.float)\n",
    "Y_val_for_test = torch.tensor(np.concatenate((Y_val[500:1000], Y_val[1500:])), dtype=torch.float)\n",
    "\n",
    "\n",
    "lr = LogisticRegression(random_state=0).fit(X_train, Y_train)\n",
    "y_pred = lr.predict_proba(X_train)[:, 1]\n",
    "y_pred.round()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-11.96437576]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03915799,  0.02396634, -0.09159853, -0.00617044, -0.00793493]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lr.intercept_)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.tensor([-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1321, -0.1382, -0.2071, -0.3147, -0.3481]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0024,  0.1673,  0.3857, -0.1583, -0.0129]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1928], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-11.9640], requires_grad=True)\n",
      "tensor([ 0.0392,  0.0240, -0.0916, -0.0062, -0.0079], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(5, 1)\n",
    "std_weight = torch.tensor([0.03915799,  0.02396634, -0.09159853, -0.00617044, -0.00793493], requires_grad=True)\n",
    "std_bias = torch.tensor([-11.964], requires_grad=True)\n",
    "with torch.no_grad():  \n",
    "    print(model.linear.weight)\n",
    "    print(model.linear.bias)\n",
    "    model.linear.weight[0] = std_weight\n",
    "    model.linear.bias[0] = std_bias\n",
    "    print(model.linear.bias)\n",
    "    print(model.linear.weight[0])\n",
    "    # model.weight = torch.nn.Parameter(torch.ones_like(model.weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -24.1423,  -35.2412,  -77.2508, -150.7342, -535.7014],\n",
       "        [  -6.9233,  -29.1268,  -66.2837, -133.0634, -290.9829],\n",
       "        [  -5.4603,  -25.4157,  -57.1742, -132.8230, -447.3622],\n",
       "        ...,\n",
       "        [  -9.5535,  -53.6929, -191.0907, -182.7891, -742.5851],\n",
       "        [  -8.3014,  -56.0449, -200.5829, -194.7397, -725.3740],\n",
       "        [  -5.5292,  -40.1784, -150.4286, -355.8837, -886.5150]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18213072423175303"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = -24*0.0392-35*0.024+0.0916*77.2508+150.7432*0.0062+0.0079*535.7014 - 11.964\n",
    "1/(1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8286e-01],\n",
       "        [2.3392e-02],\n",
       "        [3.9906e-02],\n",
       "        [4.3836e-01],\n",
       "        [6.1568e-02],\n",
       "        [1.2224e-02],\n",
       "        [2.9531e-01],\n",
       "        [6.2023e-03],\n",
       "        [1.6511e-03],\n",
       "        [8.7593e-04],\n",
       "        [3.1930e-03],\n",
       "        [5.6098e-03],\n",
       "        [1.4373e-02],\n",
       "        [7.6628e-03],\n",
       "        [2.5856e-03],\n",
       "        [4.7609e-01],\n",
       "        [4.1080e-02],\n",
       "        [2.2722e-02],\n",
       "        [9.2430e-04],\n",
       "        [5.0291e-03],\n",
       "        [6.7742e-02],\n",
       "        [8.8321e-03],\n",
       "        [1.3789e-01],\n",
       "        [3.5793e-03],\n",
       "        [3.5498e-02],\n",
       "        [9.1221e-01],\n",
       "        [6.3582e-02],\n",
       "        [4.4090e-02],\n",
       "        [6.7701e-03],\n",
       "        [1.4116e-02],\n",
       "        [6.9305e-03],\n",
       "        [1.2394e-02],\n",
       "        [2.1253e-02],\n",
       "        [2.0491e-02],\n",
       "        [4.4566e-04],\n",
       "        [2.8461e-01],\n",
       "        [8.9467e-03],\n",
       "        [1.6125e-02],\n",
       "        [4.7083e-02],\n",
       "        [2.7872e-02],\n",
       "        [5.3929e-02],\n",
       "        [3.0015e-02],\n",
       "        [2.3930e-02],\n",
       "        [9.0503e-04],\n",
       "        [5.6254e-03],\n",
       "        [1.0098e-03],\n",
       "        [4.8793e-03],\n",
       "        [3.9653e-01],\n",
       "        [1.4120e-03],\n",
       "        [1.2857e-01],\n",
       "        [3.6442e-01],\n",
       "        [7.8120e-03],\n",
       "        [1.5819e-01],\n",
       "        [3.0311e-01],\n",
       "        [2.4190e-03],\n",
       "        [7.9757e-04],\n",
       "        [1.0301e-02],\n",
       "        [3.3330e-02],\n",
       "        [4.9782e-01],\n",
       "        [1.3541e-01],\n",
       "        [1.2849e-03],\n",
       "        [3.3153e-03],\n",
       "        [1.3976e-02],\n",
       "        [2.1955e-01],\n",
       "        [1.5853e-03],\n",
       "        [2.6741e-02],\n",
       "        [2.6252e-02],\n",
       "        [2.8146e-02],\n",
       "        [4.5525e-02],\n",
       "        [1.5947e-03],\n",
       "        [2.2366e-02],\n",
       "        [2.4932e-03],\n",
       "        [2.4917e-03],\n",
       "        [3.6576e-04],\n",
       "        [3.7260e-02],\n",
       "        [3.1786e-02],\n",
       "        [7.5789e-03],\n",
       "        [5.8801e-03],\n",
       "        [2.6048e-02],\n",
       "        [6.6198e-04],\n",
       "        [5.8447e-04],\n",
       "        [1.8564e-02],\n",
       "        [4.6465e-03],\n",
       "        [4.9597e-03],\n",
       "        [1.5546e-03],\n",
       "        [2.1805e-02],\n",
       "        [1.5696e-01],\n",
       "        [1.9697e-03],\n",
       "        [2.0291e-02],\n",
       "        [1.2885e-03],\n",
       "        [1.6265e-02],\n",
       "        [4.5975e-02],\n",
       "        [9.3241e-04],\n",
       "        [4.6803e-03],\n",
       "        [6.1419e-03],\n",
       "        [3.5890e-01],\n",
       "        [6.2920e-02],\n",
       "        [1.1388e-02],\n",
       "        [9.0605e-02],\n",
       "        [8.5780e-03],\n",
       "        [6.5713e-04],\n",
       "        [4.9759e-03],\n",
       "        [4.2542e-02],\n",
       "        [5.1928e-03],\n",
       "        [2.1774e-02],\n",
       "        [4.9614e-03],\n",
       "        [1.3506e-01],\n",
       "        [2.1558e-03],\n",
       "        [8.8760e-01],\n",
       "        [1.7408e-02],\n",
       "        [4.4133e-04],\n",
       "        [6.2690e-02],\n",
       "        [2.9571e-01],\n",
       "        [4.5227e-03],\n",
       "        [6.7670e-03],\n",
       "        [5.8045e-03],\n",
       "        [5.3919e-03],\n",
       "        [3.2124e-01],\n",
       "        [6.0331e-02],\n",
       "        [3.2508e-02],\n",
       "        [1.0388e-03],\n",
       "        [1.5095e-02],\n",
       "        [1.0074e-02],\n",
       "        [4.6764e-03],\n",
       "        [1.0344e-03],\n",
       "        [8.1294e-01],\n",
       "        [1.4269e-03],\n",
       "        [4.5363e-03],\n",
       "        [4.9812e-02],\n",
       "        [9.0231e-02],\n",
       "        [2.6845e-03],\n",
       "        [6.1587e-03],\n",
       "        [2.4908e-02],\n",
       "        [2.2339e-03],\n",
       "        [1.1505e-02],\n",
       "        [1.4042e-02],\n",
       "        [5.4390e-03],\n",
       "        [3.9391e-01],\n",
       "        [7.2386e-02],\n",
       "        [8.7119e-03],\n",
       "        [4.9062e-03],\n",
       "        [2.7087e-02],\n",
       "        [4.3764e-03],\n",
       "        [5.3751e-01],\n",
       "        [8.5363e-03],\n",
       "        [4.3116e-01],\n",
       "        [9.2639e-02],\n",
       "        [3.7830e-01],\n",
       "        [1.6468e-02],\n",
       "        [2.2459e-01],\n",
       "        [5.7530e-03],\n",
       "        [2.9947e-02],\n",
       "        [7.4255e-03],\n",
       "        [1.3333e-03],\n",
       "        [3.2414e-03],\n",
       "        [8.7111e-03],\n",
       "        [5.4167e-03],\n",
       "        [2.0836e-03],\n",
       "        [4.3834e-02],\n",
       "        [2.6333e-03],\n",
       "        [2.2939e-01],\n",
       "        [1.6944e-02],\n",
       "        [5.3749e-01],\n",
       "        [1.5773e-02],\n",
       "        [1.2334e-01],\n",
       "        [1.6459e-03],\n",
       "        [1.3989e-03],\n",
       "        [1.1959e-03],\n",
       "        [3.8815e-01],\n",
       "        [9.3783e-02],\n",
       "        [1.1739e-02],\n",
       "        [2.9583e-01],\n",
       "        [6.6861e-04],\n",
       "        [2.6278e-03],\n",
       "        [2.7923e-03],\n",
       "        [5.5097e-04],\n",
       "        [2.1949e-02],\n",
       "        [1.1657e-02],\n",
       "        [1.3743e-01],\n",
       "        [1.4319e-02],\n",
       "        [9.1366e-02],\n",
       "        [7.5523e-03],\n",
       "        [4.5903e-01],\n",
       "        [2.2091e-01],\n",
       "        [1.4040e-02],\n",
       "        [1.3087e-03],\n",
       "        [1.5319e-03],\n",
       "        [3.6527e-03],\n",
       "        [1.2348e-01],\n",
       "        [2.3035e-02],\n",
       "        [1.2609e-01],\n",
       "        [3.1171e-03],\n",
       "        [8.3161e-01],\n",
       "        [5.9758e-03],\n",
       "        [1.2865e-02],\n",
       "        [1.3708e-01],\n",
       "        [1.0602e-03],\n",
       "        [6.4342e-01],\n",
       "        [4.3891e-03],\n",
       "        [1.9584e-03],\n",
       "        [3.0651e-03],\n",
       "        [3.4453e-03],\n",
       "        [1.5666e-03],\n",
       "        [8.2568e-01],\n",
       "        [2.2787e-02],\n",
       "        [1.7318e-01],\n",
       "        [2.4116e-03],\n",
       "        [4.0788e-03],\n",
       "        [5.8923e-03],\n",
       "        [4.4718e-03],\n",
       "        [6.1034e-03],\n",
       "        [8.5477e-01],\n",
       "        [1.2055e-03],\n",
       "        [3.4763e-01],\n",
       "        [2.2156e-01],\n",
       "        [1.3969e-02],\n",
       "        [3.3216e-03],\n",
       "        [6.0040e-04],\n",
       "        [8.2713e-03],\n",
       "        [2.4653e-02],\n",
       "        [5.2142e-03],\n",
       "        [3.9430e-02],\n",
       "        [5.6374e-03],\n",
       "        [4.0203e-03],\n",
       "        [1.5060e-01],\n",
       "        [2.4521e-02],\n",
       "        [4.0364e-01],\n",
       "        [2.3834e-03],\n",
       "        [1.4580e-02],\n",
       "        [1.5683e-02],\n",
       "        [1.7234e-03],\n",
       "        [3.5163e-03],\n",
       "        [3.7332e-02],\n",
       "        [2.9740e-02],\n",
       "        [6.5702e-04],\n",
       "        [3.2461e-03],\n",
       "        [4.7658e-03],\n",
       "        [4.9355e-01],\n",
       "        [1.3443e-02],\n",
       "        [8.1843e-02],\n",
       "        [1.3537e-02],\n",
       "        [1.3362e-03],\n",
       "        [2.7351e-03],\n",
       "        [6.9794e-03],\n",
       "        [9.0236e-02],\n",
       "        [3.6427e-02],\n",
       "        [8.0268e-01],\n",
       "        [1.2462e-03],\n",
       "        [1.1964e-01],\n",
       "        [7.8914e-01],\n",
       "        [1.0373e-02],\n",
       "        [8.0516e-02],\n",
       "        [1.6165e-02],\n",
       "        [2.4369e-03],\n",
       "        [8.0902e-04],\n",
       "        [1.7071e-03],\n",
       "        [4.3107e-03],\n",
       "        [7.0338e-03],\n",
       "        [7.6280e-01],\n",
       "        [1.3054e-01],\n",
       "        [5.1369e-03],\n",
       "        [1.0220e-02],\n",
       "        [3.0807e-03],\n",
       "        [2.9707e-02],\n",
       "        [1.8145e-01],\n",
       "        [1.0652e-03],\n",
       "        [9.6470e-02],\n",
       "        [4.7843e-03],\n",
       "        [2.3099e-03],\n",
       "        [7.8778e-03],\n",
       "        [1.1539e-02],\n",
       "        [1.8581e-02],\n",
       "        [2.0036e-02],\n",
       "        [5.7667e-03],\n",
       "        [8.2635e-04],\n",
       "        [2.1931e-01],\n",
       "        [1.7359e-01],\n",
       "        [8.5021e-01],\n",
       "        [1.2646e-02],\n",
       "        [7.7258e-03],\n",
       "        [2.5622e-02],\n",
       "        [9.9751e-01],\n",
       "        [1.0635e-03],\n",
       "        [1.8861e-03],\n",
       "        [2.2178e-03],\n",
       "        [8.1581e-03],\n",
       "        [4.5497e-02],\n",
       "        [8.8485e-02],\n",
       "        [4.9762e-02],\n",
       "        [8.5008e-04],\n",
       "        [1.0138e-02],\n",
       "        [2.2090e-03],\n",
       "        [2.6378e-03],\n",
       "        [4.9421e-03],\n",
       "        [1.4455e-02],\n",
       "        [1.1914e-02],\n",
       "        [1.7552e-01],\n",
       "        [3.4659e-04],\n",
       "        [2.3693e-01],\n",
       "        [5.4056e-03],\n",
       "        [2.5434e-03],\n",
       "        [2.7576e-03],\n",
       "        [2.2968e-02],\n",
       "        [2.1240e-02],\n",
       "        [2.0266e-03],\n",
       "        [2.6136e-03],\n",
       "        [6.1396e-02],\n",
       "        [2.8497e-02],\n",
       "        [1.3111e-01],\n",
       "        [1.3051e-01],\n",
       "        [2.0046e-04],\n",
       "        [1.7358e-01],\n",
       "        [1.0870e-03],\n",
       "        [1.3895e-02],\n",
       "        [2.2032e-02],\n",
       "        [7.0335e-01],\n",
       "        [3.0804e-02],\n",
       "        [7.5136e-03],\n",
       "        [6.1292e-03],\n",
       "        [2.7052e-02],\n",
       "        [2.9541e-03],\n",
       "        [7.1436e-03],\n",
       "        [5.8337e-03],\n",
       "        [2.3821e-01],\n",
       "        [3.1349e-02],\n",
       "        [9.6235e-02],\n",
       "        [7.9177e-03],\n",
       "        [1.2379e-03],\n",
       "        [4.7476e-02],\n",
       "        [4.3313e-02],\n",
       "        [1.4107e-02],\n",
       "        [1.1765e-02],\n",
       "        [3.9635e-02],\n",
       "        [2.4649e-03],\n",
       "        [2.2619e-03],\n",
       "        [1.7442e-02],\n",
       "        [3.3335e-03],\n",
       "        [4.6620e-04],\n",
       "        [3.4207e-02],\n",
       "        [6.0564e-02],\n",
       "        [9.4087e-01],\n",
       "        [4.0345e-01],\n",
       "        [6.1490e-02],\n",
       "        [2.3852e-02],\n",
       "        [1.0116e-02],\n",
       "        [2.5849e-01],\n",
       "        [4.0160e-02],\n",
       "        [8.1055e-01],\n",
       "        [1.5824e-01],\n",
       "        [7.7638e-02],\n",
       "        [2.1732e-03],\n",
       "        [5.9547e-03],\n",
       "        [6.3327e-02],\n",
       "        [1.5186e-03],\n",
       "        [2.9488e-02],\n",
       "        [8.4516e-01],\n",
       "        [5.2978e-01],\n",
       "        [2.4274e-02],\n",
       "        [7.0748e-04],\n",
       "        [3.2226e-03],\n",
       "        [7.2148e-03],\n",
       "        [1.6539e-03],\n",
       "        [3.2234e-03],\n",
       "        [1.8856e-02],\n",
       "        [3.6958e-03],\n",
       "        [5.4191e-03],\n",
       "        [4.3152e-01],\n",
       "        [3.1750e-01],\n",
       "        [3.3216e-01],\n",
       "        [1.6115e-03],\n",
       "        [1.1905e-01],\n",
       "        [1.7097e-03],\n",
       "        [1.0078e-03],\n",
       "        [2.3776e-03],\n",
       "        [2.4006e-03],\n",
       "        [1.2046e-02],\n",
       "        [1.9232e-02],\n",
       "        [4.3790e-03],\n",
       "        [5.4085e-02],\n",
       "        [2.9103e-03],\n",
       "        [8.7621e-03],\n",
       "        [4.0441e-03],\n",
       "        [8.8364e-04],\n",
       "        [6.5840e-03],\n",
       "        [4.8717e-03],\n",
       "        [3.1633e-02],\n",
       "        [2.1793e-03],\n",
       "        [4.5009e-02],\n",
       "        [7.7831e-02],\n",
       "        [7.2948e-03],\n",
       "        [3.3885e-03],\n",
       "        [3.5922e-03],\n",
       "        [8.6429e-02],\n",
       "        [1.2886e-01],\n",
       "        [7.8715e-03],\n",
       "        [1.7375e-01],\n",
       "        [8.8720e-03],\n",
       "        [4.9207e-03],\n",
       "        [3.0527e-02],\n",
       "        [5.8461e-03],\n",
       "        [6.0617e-03],\n",
       "        [2.2010e-01],\n",
       "        [9.7850e-03],\n",
       "        [1.7017e-03],\n",
       "        [4.0164e-04],\n",
       "        [9.5688e-01],\n",
       "        [1.8730e-03],\n",
       "        [2.4985e-03],\n",
       "        [6.2790e-03],\n",
       "        [4.1765e-02],\n",
       "        [6.6221e-02],\n",
       "        [6.9308e-03],\n",
       "        [1.2308e-01],\n",
       "        [1.7935e-02],\n",
       "        [1.0490e-02],\n",
       "        [8.4902e-04],\n",
       "        [2.8470e-02],\n",
       "        [1.9404e-01],\n",
       "        [1.5998e-01],\n",
       "        [1.0545e-02],\n",
       "        [3.5498e-03],\n",
       "        [3.3232e-03],\n",
       "        [9.6106e-03],\n",
       "        [1.1113e-01],\n",
       "        [3.7361e-02],\n",
       "        [2.4735e-03],\n",
       "        [1.7852e-01],\n",
       "        [6.9101e-03],\n",
       "        [2.0568e-01],\n",
       "        [2.1592e-02],\n",
       "        [8.4051e-02],\n",
       "        [9.8970e-03],\n",
       "        [1.0560e-02],\n",
       "        [6.3131e-03],\n",
       "        [3.4892e-02],\n",
       "        [1.8603e-03],\n",
       "        [1.9486e-02],\n",
       "        [7.6322e-03],\n",
       "        [2.4971e-01],\n",
       "        [1.2404e-02],\n",
       "        [4.2970e-03],\n",
       "        [3.5470e-03],\n",
       "        [7.6060e-04],\n",
       "        [2.6487e-02],\n",
       "        [3.7716e-03],\n",
       "        [4.1296e-03],\n",
       "        [1.9236e-03],\n",
       "        [1.6705e-03],\n",
       "        [9.2969e-03],\n",
       "        [5.3137e-02],\n",
       "        [2.6310e-02],\n",
       "        [1.9767e-03],\n",
       "        [4.1705e-03],\n",
       "        [1.1123e-02],\n",
       "        [3.0089e-03],\n",
       "        [1.6439e-02],\n",
       "        [9.1086e-02],\n",
       "        [4.2109e-03],\n",
       "        [2.0045e-02],\n",
       "        [3.8137e-02],\n",
       "        [1.9234e-02],\n",
       "        [5.5396e-03],\n",
       "        [1.9906e-03],\n",
       "        [3.7329e-03],\n",
       "        [4.3060e-02],\n",
       "        [9.7047e-04],\n",
       "        [2.2032e-03],\n",
       "        [5.0032e-02],\n",
       "        [3.0937e-01],\n",
       "        [1.0647e-02],\n",
       "        [2.2889e-01],\n",
       "        [1.1043e-02],\n",
       "        [1.3091e-03],\n",
       "        [5.7757e-03],\n",
       "        [2.7728e-02],\n",
       "        [1.1514e-02],\n",
       "        [2.0165e-02],\n",
       "        [3.3013e-02],\n",
       "        [4.7291e-02],\n",
       "        [1.5384e-02],\n",
       "        [2.8689e-02],\n",
       "        [4.9491e-03],\n",
       "        [1.5375e-03],\n",
       "        [2.8384e-01],\n",
       "        [5.4484e-03],\n",
       "        [3.5353e-02],\n",
       "        [6.8703e-04],\n",
       "        [1.8285e-02],\n",
       "        [1.8745e-02],\n",
       "        [7.2977e-03],\n",
       "        [8.4911e-03],\n",
       "        [2.3257e-01],\n",
       "        [7.0672e-04],\n",
       "        [1.4738e-03],\n",
       "        [2.2062e-03],\n",
       "        [1.0459e-03],\n",
       "        [1.0294e-02],\n",
       "        [1.1817e-02],\n",
       "        [4.7569e-03],\n",
       "        [2.3482e-03],\n",
       "        [9.9904e-01],\n",
       "        [9.9681e-01],\n",
       "        [9.9999e-01],\n",
       "        [9.6084e-01],\n",
       "        [9.9994e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9837e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9325e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9991e-01],\n",
       "        [9.9991e-01],\n",
       "        [9.7316e-01],\n",
       "        [9.9969e-01],\n",
       "        [9.9971e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.1670e-01],\n",
       "        [8.0746e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9975e-01],\n",
       "        [9.9993e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9606e-01],\n",
       "        [9.9998e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9998e-01],\n",
       "        [9.9757e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9994e-01],\n",
       "        [9.9987e-01],\n",
       "        [9.9995e-01],\n",
       "        [9.9973e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9966e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.4728e-01],\n",
       "        [7.8609e-01],\n",
       "        [9.9876e-01],\n",
       "        [9.9983e-01],\n",
       "        [9.9970e-01],\n",
       "        [9.9011e-01],\n",
       "        [9.8170e-01],\n",
       "        [9.9993e-01],\n",
       "        [9.9985e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [7.0302e-01],\n",
       "        [3.9637e-01],\n",
       "        [7.3445e-01],\n",
       "        [3.4441e-01],\n",
       "        [9.9803e-01],\n",
       "        [9.0720e-01],\n",
       "        [9.1566e-01],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [9.0000e-01],\n",
       "        [4.4306e-01],\n",
       "        [9.9398e-01],\n",
       "        [9.5711e-01],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9969e-01],\n",
       "        [9.9995e-01],\n",
       "        [9.9985e-01],\n",
       "        [9.9660e-01],\n",
       "        [9.9996e-01],\n",
       "        [9.9882e-01],\n",
       "        [9.9959e-01],\n",
       "        [9.9379e-01],\n",
       "        [9.9997e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.5553e-01],\n",
       "        [9.9978e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.0395e-01],\n",
       "        [9.8468e-01],\n",
       "        [9.1166e-01],\n",
       "        [9.9943e-01],\n",
       "        [9.9980e-01],\n",
       "        [9.9785e-01],\n",
       "        [9.9990e-01],\n",
       "        [9.9997e-01],\n",
       "        [9.1438e-01],\n",
       "        [9.7069e-01],\n",
       "        [9.9970e-01],\n",
       "        [9.9907e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9984e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9997e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9991e-01],\n",
       "        [9.7939e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9992e-01],\n",
       "        [9.9999e-01],\n",
       "        [9.9980e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [4.5181e-01],\n",
       "        [9.1685e-01],\n",
       "        [9.9139e-01],\n",
       "        [9.9891e-01],\n",
       "        [9.9034e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9996e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9992e-01],\n",
       "        [9.9997e-01],\n",
       "        [9.3323e-01],\n",
       "        [8.0519e-01],\n",
       "        [8.3724e-01],\n",
       "        [3.4875e-01],\n",
       "        [9.9838e-01],\n",
       "        [9.9523e-01],\n",
       "        [8.3266e-01],\n",
       "        [9.9969e-01],\n",
       "        [9.9919e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [4.9943e-01],\n",
       "        [9.5798e-01],\n",
       "        [9.9968e-01],\n",
       "        [9.9999e-01],\n",
       "        [2.0568e-01],\n",
       "        [3.0030e-02],\n",
       "        [8.7796e-01],\n",
       "        [9.4663e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [6.1026e-01],\n",
       "        [9.6388e-01],\n",
       "        [9.8128e-01],\n",
       "        [9.9954e-01],\n",
       "        [9.9999e-01],\n",
       "        [9.9318e-01],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9982e-01],\n",
       "        [9.9456e-01],\n",
       "        [5.9629e-01],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.7963e-01],\n",
       "        [9.9638e-01],\n",
       "        [9.9561e-01],\n",
       "        [9.6147e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9998e-01],\n",
       "        [9.8598e-01],\n",
       "        [9.3504e-01],\n",
       "        [9.9985e-01],\n",
       "        [9.9906e-01],\n",
       "        [9.9969e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [9.9892e-01],\n",
       "        [9.9427e-01],\n",
       "        [9.3284e-01],\n",
       "        [9.6266e-01],\n",
       "        [9.5605e-01],\n",
       "        [9.9946e-01],\n",
       "        [9.9729e-01],\n",
       "        [9.9695e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9998e-01],\n",
       "        [9.9996e-01],\n",
       "        [9.9998e-01],\n",
       "        [9.9579e-01],\n",
       "        [9.9821e-01],\n",
       "        [9.9895e-01],\n",
       "        [1.0000e+00],\n",
       "        [7.3068e-01],\n",
       "        [2.1773e-01],\n",
       "        [9.9994e-01],\n",
       "        [1.0000e+00],\n",
       "        [7.0564e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [9.9946e-01],\n",
       "        [9.9996e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9981e-01],\n",
       "        [9.9999e-01],\n",
       "        [4.9392e-01],\n",
       "        [7.3140e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [9.9995e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [8.9251e-01],\n",
       "        [9.9411e-01],\n",
       "        [5.3789e-01],\n",
       "        [9.9809e-01],\n",
       "        [8.2702e-01],\n",
       "        [9.0160e-01],\n",
       "        [8.1237e-01],\n",
       "        [9.9782e-01],\n",
       "        [9.9827e-01],\n",
       "        [9.9958e-01],\n",
       "        [8.4373e-01],\n",
       "        [9.9977e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [2.1333e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9987e-01],\n",
       "        [9.9992e-01],\n",
       "        [9.9998e-01],\n",
       "        [9.9995e-01],\n",
       "        [9.9994e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [3.3298e-01],\n",
       "        [1.0745e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [8.9121e-01],\n",
       "        [9.9973e-01],\n",
       "        [9.9987e-01],\n",
       "        [9.5148e-01],\n",
       "        [7.0092e-01],\n",
       "        [9.9996e-01],\n",
       "        [9.9979e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9997e-01],\n",
       "        [9.4897e-01],\n",
       "        [9.1778e-01],\n",
       "        [8.6305e-01],\n",
       "        [9.9963e-01],\n",
       "        [9.9954e-01],\n",
       "        [9.9421e-01],\n",
       "        [9.9657e-01],\n",
       "        [9.1347e-01],\n",
       "        [9.9954e-01],\n",
       "        [1.9730e-01],\n",
       "        [9.9976e-01],\n",
       "        [9.9787e-01],\n",
       "        [9.9991e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9912e-01],\n",
       "        [9.9726e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9295e-01],\n",
       "        [9.6294e-01],\n",
       "        [9.6908e-01],\n",
       "        [9.8033e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9998e-01],\n",
       "        [9.9996e-01],\n",
       "        [9.9991e-01],\n",
       "        [9.8887e-01],\n",
       "        [8.5279e-01],\n",
       "        [5.6209e-01],\n",
       "        [6.6482e-01],\n",
       "        [3.2000e-02],\n",
       "        [4.2275e-01],\n",
       "        [9.9943e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [2.6559e-01],\n",
       "        [7.0590e-01],\n",
       "        [4.2092e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.8292e-01],\n",
       "        [9.9994e-01],\n",
       "        [9.9727e-01],\n",
       "        [9.3443e-01],\n",
       "        [9.6603e-01],\n",
       "        [9.9902e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9998e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9970e-01],\n",
       "        [9.9922e-01],\n",
       "        [9.9721e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9727e-01],\n",
       "        [9.3644e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.5870e-01],\n",
       "        [9.9556e-01],\n",
       "        [2.4472e-01],\n",
       "        [4.6322e-01],\n",
       "        [9.9995e-01],\n",
       "        [9.8601e-01],\n",
       "        [9.9984e-01],\n",
       "        [1.7375e-01],\n",
       "        [9.9987e-01],\n",
       "        [8.3857e-01],\n",
       "        [4.8946e-01],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9917e-01],\n",
       "        [9.9723e-01],\n",
       "        [9.5845e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9686e-01],\n",
       "        [9.9892e-01],\n",
       "        [9.9934e-01],\n",
       "        [9.9668e-01],\n",
       "        [9.9970e-01],\n",
       "        [9.7761e-01],\n",
       "        [9.9976e-01],\n",
       "        [6.4147e-01],\n",
       "        [4.0394e-01],\n",
       "        [9.4251e-02],\n",
       "        [9.9810e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9730e-01],\n",
       "        [9.9851e-01],\n",
       "        [9.9995e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9977e-01],\n",
       "        [9.9949e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [8.1876e-01],\n",
       "        [9.9979e-01],\n",
       "        [9.9977e-01],\n",
       "        [9.9995e-01],\n",
       "        [9.9962e-01],\n",
       "        [9.9814e-01],\n",
       "        [5.8968e-01],\n",
       "        [4.0364e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.5857e-01],\n",
       "        [9.9908e-01],\n",
       "        [9.7261e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9997e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.1678e-01],\n",
       "        [9.8291e-01],\n",
       "        [9.9789e-01],\n",
       "        [9.9970e-01],\n",
       "        [9.5459e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.8138e-01],\n",
       "        [9.7402e-01],\n",
       "        [9.9117e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [7.9208e-01],\n",
       "        [3.9935e-01],\n",
       "        [4.9049e-01],\n",
       "        [4.0850e-01],\n",
       "        [9.8214e-01],\n",
       "        [9.9946e-01],\n",
       "        [9.9835e-01],\n",
       "        [9.9968e-01],\n",
       "        [9.9999e-01],\n",
       "        [9.9986e-01],\n",
       "        [9.9974e-01],\n",
       "        [9.9757e-01],\n",
       "        [4.6521e-01],\n",
       "        [5.7645e-01],\n",
       "        [5.1857e-01],\n",
       "        [9.5213e-01],\n",
       "        [9.9843e-01],\n",
       "        [7.8835e-01],\n",
       "        [5.3395e-01],\n",
       "        [8.8962e-01],\n",
       "        [9.7612e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.2260e-01],\n",
       "        [5.3271e-01],\n",
       "        [9.9985e-01],\n",
       "        [9.9997e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9737e-01],\n",
       "        [9.9927e-01],\n",
       "        [9.6453e-01],\n",
       "        [9.5661e-01],\n",
       "        [9.9993e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [9.9977e-01],\n",
       "        [9.9993e-01],\n",
       "        [9.9923e-01],\n",
       "        [9.9872e-01],\n",
       "        [9.6596e-01],\n",
       "        [9.9419e-01],\n",
       "        [9.9997e-01],\n",
       "        [9.9998e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9826e-01],\n",
       "        [5.1677e-01],\n",
       "        [9.9405e-01],\n",
       "        [8.5374e-01],\n",
       "        [8.1573e-01],\n",
       "        [9.9873e-01],\n",
       "        [9.9985e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.8546e-01],\n",
       "        [9.9988e-01],\n",
       "        [9.9983e-01],\n",
       "        [9.9895e-01],\n",
       "        [9.5006e-01],\n",
       "        [9.9982e-01],\n",
       "        [9.9857e-01],\n",
       "        [9.9918e-01],\n",
       "        [9.8520e-01],\n",
       "        [9.8573e-01],\n",
       "        [9.7139e-01],\n",
       "        [9.9872e-01],\n",
       "        [9.9993e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [8.0540e-01],\n",
       "        [9.9283e-01],\n",
       "        [5.0081e-01],\n",
       "        [5.3943e-01],\n",
       "        [2.3949e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9940e-01],\n",
       "        [9.9947e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9996e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [9.9996e-01],\n",
       "        [9.9994e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9920e-01],\n",
       "        [9.2733e-01],\n",
       "        [5.0877e-01],\n",
       "        [9.9986e-01],\n",
       "        [9.9529e-01],\n",
       "        [1.0933e-01],\n",
       "        [2.9455e-01],\n",
       "        [9.8387e-01],\n",
       "        [9.9982e-01],\n",
       "        [9.9634e-01],\n",
       "        [9.9196e-01],\n",
       "        [1.0000e+00],\n",
       "        [6.8760e-01],\n",
       "        [9.9925e-01],\n",
       "        [9.9217e-01],\n",
       "        [9.9996e-01],\n",
       "        [9.9871e-01],\n",
       "        [9.9969e-01],\n",
       "        [9.9620e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9974e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9998e-01],\n",
       "        [9.9999e-01],\n",
       "        [9.9995e-01]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2443,  0.0947, -0.0014, -0.0274,  0.5002]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3317], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Loss: 4.412589073181152. Accuracy: 50.0\n",
      "Train -  Loss: 3.9846997261047363. Accuracy: 50.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 100/5000 [00:06<05:43, 14.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Loss: 4.412589073181152. Accuracy: 50.0\n",
      "Train -  Loss: 3.119951009750366. Accuracy: 50.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 146/5000 [00:10<05:34, 14.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zjiaming/deep_Mahalanobis_detector/test_oodreducer.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnioru.eecs.umich.edu/home/zjiaming/deep_Mahalanobis_detector/test_oodreducer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Y_train = Variable(Y_train)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnioru.eecs.umich.edu/home/zjiaming/deep_Mahalanobis_detector/test_oodreducer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(epochs))):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnioru.eecs.umich.edu/home/zjiaming/deep_Mahalanobis_detector/test_oodreducer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mfor\u001b[39;00m data_batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnioru.eecs.umich.edu/home/zjiaming/deep_Mahalanobis_detector/test_oodreducer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         data \u001b[39m=\u001b[39m Variable(data_batch[\u001b[39m'\u001b[39m\u001b[39mConfidence\u001b[39m\u001b[39m'\u001b[39m], requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnioru.eecs.umich.edu/home/zjiaming/deep_Mahalanobis_detector/test_oodreducer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         labels \u001b[39m=\u001b[39m data_batch[\u001b[39m'\u001b[39m\u001b[39mClass\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/randomTexture/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/randomTexture/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1358\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1359\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1362\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/randomTexture/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1324\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1325\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1326\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1327\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/randomTexture/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1151\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1164\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1165\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/randomTexture/lib/python3.10/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[39m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[39mreturn\u001b[39;00m _ForkingPickler\u001b[39m.\u001b[39;49mloads(res)\n",
      "File \u001b[0;32m~/anaconda3/envs/randomTexture/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:297\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrebuild_storage_fd\u001b[39m(\u001b[39mcls\u001b[39m, df, size):\n\u001b[0;32m--> 297\u001b[0m     fd \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mdetach()\n\u001b[1;32m    298\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m         storage \u001b[39m=\u001b[39m storage_from_cache(\u001b[39mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m~/anaconda3/envs/randomTexture/lib/python3.10/multiprocessing/resource_sharer.py:58\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mwith\u001b[39;00m _resource_sharer\u001b[39m.\u001b[39mget_connection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id) \u001b[39mas\u001b[39;00m conn:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m reduction\u001b[39m.\u001b[39;49mrecv_handle(conn)\n",
      "File \u001b[0;32m~/anaconda3/envs/randomTexture/lib/python3.10/multiprocessing/reduction.py:189\u001b[0m, in \u001b[0;36mrecv_handle\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39m'''Receive a handle over a local connection.'''\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39mwith\u001b[39;00m socket\u001b[39m.\u001b[39mfromfd(conn\u001b[39m.\u001b[39mfileno(), socket\u001b[39m.\u001b[39mAF_UNIX, socket\u001b[39m.\u001b[39mSOCK_STREAM) \u001b[39mas\u001b[39;00m s:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m recvfds(s, \u001b[39m1\u001b[39;49m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/randomTexture/lib/python3.10/multiprocessing/reduction.py:157\u001b[0m, in \u001b[0;36mrecvfds\u001b[0;34m(sock, size)\u001b[0m\n\u001b[1;32m    155\u001b[0m a \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39marray(\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    156\u001b[0m bytes_size \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mitemsize \u001b[39m*\u001b[39m size\n\u001b[0;32m--> 157\u001b[0m msg, ancdata, flags, addr \u001b[39m=\u001b[39m sock\u001b[39m.\u001b[39;49mrecvmsg(\u001b[39m1\u001b[39;49m, socket\u001b[39m.\u001b[39;49mCMSG_SPACE(bytes_size))\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m msg \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ancdata:\n\u001b[1;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "import torch\n",
    "# class LogisticRegression(torch.nn.Module):\n",
    "#      def __init__(self, input_dim, output_dim):\n",
    "#          super(LogisticRegression, self).__init__()\n",
    "#          self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "         \n",
    "#      def forward(self, x):\n",
    "#          outputs = torch.sigmoid(self.linear(x))\n",
    "#          return outputs\n",
    "# model = LogisticRegression(5, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "\n",
    "# Y_train = Variable(Y_train)\n",
    "for epoch in tqdm(range(int(epochs))):\n",
    "    for data_batch in train_loader:\n",
    "        data = Variable(data_batch['Confidence'], requires_grad=True)\n",
    "        labels = data_batch['Class']\n",
    "        # print(data)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        # outputs = outputs.view(outputs.size(0))\n",
    "        # print(outputs.shape)\n",
    "        loss = criterion(outputs, labels) \n",
    "        # print(loss)\n",
    "        # break\n",
    "        # loss = Variable(loss, requires_grad=True)\n",
    "        # loss.backward()\n",
    "        \n",
    "        # optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            total_test = 0\n",
    "            correct_test = 0\n",
    "            output_test = torch.squeeze(model(X_val_for_test))\n",
    "            loss_test = criterion(output_test, Y_val_for_test)\n",
    "\n",
    "            predicted_test = output_test.round().detach().numpy()\n",
    "            total_test += Y_val_for_test.size(0)\n",
    "\n",
    "            correct_test += np.sum(predicted_test == Y_val_for_test.detach().numpy())\n",
    "\n",
    "            accuracy_test = 100 * correct_test/total_test\n",
    "\n",
    "            print(f\"Test - Loss: {loss_test.item()}. Accuracy: {accuracy_test}\")\n",
    "\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            total += Y_train.size(0)\n",
    "            \n",
    "            correct += np.sum(torch.squeeze(model(X_train)).round().detach().numpy() == Y_train.detach().numpy())\n",
    "            accuracy = 100 * correct/total\n",
    "            print(f\"Train -  Loss: {loss.item()}. Accuracy: {accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zjiaming/anaconda3/envs/randomTexture/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zjiaming/deep_Mahalanobis_detector/lib_generation.py:69: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data = Variable(data, volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Accuracy:(99.75%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "from torchvision import transforms\n",
    "import data_loader\n",
    "import lib_generation\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = models.ResNet34(num_c=10)\n",
    "model.load_state_dict(torch.load(\"pre_trained/resnet_cifar10.pth\", map_location = \"cuda:\" + str(0)))\n",
    "in_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
    "\n",
    "train_loader, test_loader = data_loader.getTargetDataSet('cifar10', 16, in_transform, './data')\n",
    "\n",
    "model.cuda()\n",
    "# set information about feature extaction\n",
    "model.eval()\n",
    "temp_x = torch.rand(2,3,32,32).cuda()\n",
    "temp_x = Variable(temp_x)\n",
    "temp_list = model.feature_list(temp_x)[1]\n",
    "num_output = len(temp_list)\n",
    "feature_list = np.empty(num_output)\n",
    "count = 0\n",
    "for out in temp_list:\n",
    "    feature_list[count] = out.size(1)\n",
    "    count += 1\n",
    "\n",
    "sample_mean, precision = lib_generation.sample_estimator(model, 10, feature_list, train_loader)\n",
    "\n",
    "outf = './output/resnet_cifar10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC(torch.nn.Module):\n",
    "    def __init__(self, model, num_classes, outf, out_flag, net_type, sample_mean, precision, layer_index, magnitude):\n",
    "        super(AC, self).__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.layer_index = layer_index\n",
    "        self.sample_mean = sample_mean\n",
    "        self.precision = precision\n",
    "        self.num_classes = num_classes\n",
    "        self.layers = layer_index\n",
    "\n",
    "        self.linear = torch.nn.Linear(5, 1).cuda()\n",
    "\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        confidence_score = 0\n",
    "        for layers in range(self.layers):\n",
    "            out_features = self.model.intermediate_forward(x, layers)\n",
    "            out_features = out_features.view(out_features.size(0), out_features.size(1), -1)\n",
    "            out_features = torch.mean(out_features, 2)\n",
    "\n",
    "            # compute Mahalanobis score\n",
    "            gaussian_score = 0\n",
    "            for i in range(self.num_classes):\n",
    "                batch_sample_mean = self.sample_mean[layers][i]\n",
    "                # zero_f = out_features.data - batch_sample_mean\n",
    "                zero_f = out_features - batch_sample_mean\n",
    "                term_gau = -0.5*torch.mm(torch.mm(zero_f, self.precision[layers]), zero_f.t()).diag()\n",
    "                if i == 0:\n",
    "                    gaussian_score = term_gau.view(-1,1)\n",
    "                else:\n",
    "                    gaussian_score = torch.cat((gaussian_score, term_gau.view(-1,1)), 1)\n",
    "            \n",
    "            gaussian_score, _ = torch.torch.max(gaussian_score, dim=1)\n",
    "            \n",
    "            gaussian_score = torch.unsqueeze(gaussian_score, dim=1)\n",
    "            if layers == 0:\n",
    "                confidence_score = gaussian_score\n",
    "            else:\n",
    "                confidence_score = torch.cat((confidence_score, gaussian_score), 1)\n",
    "\n",
    "        outputs = torch.sigmoid(self.linear(confidence_score))\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = AC(model, 10, outf, True, 'resnet', sample_mean, precision, 5, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3531,  0.1318, -0.0377,  0.0889, -0.2183]], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# for param in d_model.parameters():\n",
    "#     print(param)\n",
    "std_weight = torch.tensor([0.03915799,  0.02396634, -0.09159853, -0.00617044, -0.00793493], requires_grad=True)\n",
    "std_bias = torch.tensor([-11.964], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    print(d_model.linear.weight)\n",
    "    # print(model.linear.bias)\n",
    "    d_model.linear.weight[0] = std_weight\n",
    "    d_model.linear.bias[0] = std_bias\n",
    "    # print(model.linear.bias)\n",
    "    # print(model.linear.weight[0])\n",
    "    # model.weight = torch.nn.Parameter(torch.ones_like(model.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before tensor([[0.1829],\n",
      "        [0.0234],\n",
      "        [0.0399],\n",
      "        [0.4384],\n",
      "        [0.0616],\n",
      "        [0.0122],\n",
      "        [0.2953],\n",
      "        [0.0062],\n",
      "        [0.0017],\n",
      "        [0.0009],\n",
      "        [0.0032],\n",
      "        [0.0056],\n",
      "        [0.0144],\n",
      "        [0.0077],\n",
      "        [0.0026],\n",
      "        [0.4761]], device='cuda:0')\n",
      "before confidence tensor([[3.0283e-05, 4.2229e-05, 1.0920e-05, 9.9961e-01, 1.4067e-05, 1.3729e-04,\n",
      "         1.2954e-04, 1.1758e-05, 7.7413e-06, 5.9452e-06],\n",
      "        [2.0396e-05, 6.4483e-05, 5.0365e-06, 9.4132e-06, 4.3504e-06, 6.7227e-06,\n",
      "         1.1908e-05, 5.6047e-06, 9.9987e-01, 6.0123e-06],\n",
      "        [1.4476e-03, 7.5674e-01, 6.0230e-04, 3.8191e-04, 4.5819e-04, 5.7109e-04,\n",
      "         6.9231e-04, 5.3860e-04, 2.3446e-01, 4.1062e-03],\n",
      "        [9.8827e-01, 3.2023e-04, 5.1467e-03, 5.0750e-04, 1.5264e-04, 2.0724e-04,\n",
      "         3.4502e-04, 3.3499e-04, 5.4772e-04, 4.1680e-03],\n",
      "        [2.4646e-05, 7.7813e-05, 1.8792e-04, 6.1568e-05, 7.3602e-05, 2.7898e-05,\n",
      "         9.9947e-01, 1.7561e-05, 2.5771e-05, 2.9024e-05],\n",
      "        [3.0820e-05, 1.4786e-04, 3.7323e-05, 7.7692e-04, 7.2003e-05, 7.9486e-05,\n",
      "         9.9863e-01, 8.5206e-05, 4.6662e-05, 8.9971e-05],\n",
      "        [9.8416e-05, 9.9580e-01, 4.8798e-04, 1.3321e-04, 2.9132e-05, 6.7788e-04,\n",
      "         1.0711e-04, 4.0387e-04, 7.7201e-05, 2.1854e-03],\n",
      "        [9.8972e-05, 6.8043e-05, 7.9070e-04, 9.4284e-05, 7.2032e-05, 4.0586e-05,\n",
      "         9.9867e-01, 5.3069e-05, 2.2252e-05, 8.6411e-05],\n",
      "        [4.8297e-05, 1.9254e-05, 1.7708e-05, 9.9938e-01, 4.0291e-05, 3.7580e-04,\n",
      "         4.7766e-05, 4.3340e-05, 2.0969e-05, 1.0640e-05],\n",
      "        [9.5534e-05, 9.9588e-01, 1.0212e-04, 9.4008e-05, 8.8155e-05, 1.2483e-04,\n",
      "         1.7921e-04, 2.2675e-04, 2.5112e-04, 2.9551e-03],\n",
      "        [9.9945e-01, 3.3778e-05, 4.5899e-05, 8.5500e-05, 8.8767e-05, 6.8753e-05,\n",
      "         3.1952e-05, 4.6843e-05, 4.9949e-05, 9.5790e-05],\n",
      "        [2.1972e-05, 4.0976e-05, 1.5356e-05, 3.0594e-05, 1.1635e-05, 1.4356e-05,\n",
      "         1.4878e-05, 2.1500e-05, 7.3780e-05, 9.9975e-01],\n",
      "        [1.9304e-05, 2.6142e-05, 3.2198e-05, 3.2351e-04, 5.0408e-05, 9.9935e-01,\n",
      "         6.7327e-05, 8.1378e-05, 1.8368e-05, 2.7477e-05],\n",
      "        [8.2961e-05, 2.6066e-04, 1.5284e-04, 2.0321e-04, 9.0845e-05, 1.6353e-04,\n",
      "         1.4912e-04, 9.9869e-01, 6.5566e-05, 1.4090e-04],\n",
      "        [8.4510e-06, 1.3797e-05, 3.9055e-06, 5.2655e-06, 6.7720e-06, 8.5271e-06,\n",
      "         4.8488e-06, 6.3682e-06, 1.0335e-05, 9.9993e-01],\n",
      "        [3.4629e-04, 9.6778e-05, 2.1294e-05, 6.2021e-05, 1.6365e-04, 1.9220e-05,\n",
      "         6.3899e-04, 2.7390e-05, 9.9858e-01, 4.3027e-05]], device='cuda:0')\n",
      "after attack tensor([[9.4161e-05, 2.6780e-04, 4.4446e-05, 4.8538e-03, 1.1582e-04, 9.9423e-01,\n",
      "         2.2960e-04, 4.7388e-05, 4.0655e-05, 7.5799e-05],\n",
      "        [6.3050e-04, 5.1777e-01, 1.5690e-04, 2.3803e-04, 1.7961e-04, 2.6868e-04,\n",
      "         3.7191e-04, 2.5596e-04, 4.7975e-01, 3.7266e-04],\n",
      "        [6.0547e-05, 9.9919e-01, 5.3905e-05, 2.0275e-05, 2.5659e-05, 2.8129e-05,\n",
      "         7.9487e-05, 5.0532e-05, 3.6801e-04, 1.2368e-04],\n",
      "        [2.2067e-03, 1.2883e-04, 9.9586e-01, 1.0769e-04, 2.4853e-04, 2.6078e-04,\n",
      "         3.2045e-04, 1.6517e-04, 5.2739e-04, 1.7136e-04],\n",
      "        [2.7690e-04, 4.0078e-04, 2.4156e-03, 4.1428e-04, 1.0646e-01, 8.5846e-05,\n",
      "         8.8934e-01, 1.2777e-04, 1.8228e-04, 2.9238e-04],\n",
      "        [2.0807e-05, 1.8680e-05, 2.4631e-05, 9.9945e-01, 4.7752e-05, 8.5101e-05,\n",
      "         2.7890e-04, 5.0814e-05, 1.2735e-05, 1.1560e-05],\n",
      "        [9.3872e-05, 9.7554e-04, 3.2363e-04, 2.3631e-03, 5.5040e-05, 9.9453e-01,\n",
      "         7.3053e-05, 1.1893e-04, 2.0340e-05, 1.4483e-03],\n",
      "        [4.9589e-04, 6.0434e-05, 9.8822e-01, 4.9716e-04, 2.2658e-04, 1.0472e-04,\n",
      "         1.0126e-02, 1.2388e-04, 3.5716e-05, 1.0572e-04],\n",
      "        [4.2817e-05, 2.4734e-05, 3.7432e-05, 9.9835e-01, 1.3437e-04, 1.2356e-03,\n",
      "         9.2587e-05, 4.4173e-05, 2.4991e-05, 1.2883e-05],\n",
      "        [6.7554e-05, 1.8290e-03, 5.7864e-05, 6.7109e-05, 6.0061e-05, 7.2538e-05,\n",
      "         1.0159e-04, 1.3383e-04, 1.7059e-04, 9.9744e-01],\n",
      "        [3.6753e-04, 9.5229e-05, 1.3766e-04, 1.9444e-03, 1.4276e-03, 9.9543e-01,\n",
      "         9.7696e-05, 1.5284e-04, 1.9463e-04, 1.5400e-04],\n",
      "        [1.1638e-04, 1.2122e-04, 7.3462e-05, 1.2926e-04, 6.9271e-05, 6.3821e-05,\n",
      "         7.0864e-05, 1.3599e-04, 4.4517e-03, 9.9477e-01],\n",
      "        [2.0957e-04, 1.6065e-04, 4.8136e-04, 9.2952e-01, 3.0030e-04, 6.7721e-02,\n",
      "         1.1233e-03, 2.5279e-04, 1.0878e-04, 1.2577e-04],\n",
      "        [1.3008e-04, 1.6865e-03, 1.5991e-04, 1.9093e-04, 1.5368e-04, 3.2601e-04,\n",
      "         1.7178e-04, 9.9663e-01, 1.1746e-04, 4.3792e-04],\n",
      "        [1.7012e-04, 2.7305e-04, 8.6914e-05, 1.4662e-04, 2.2481e-04, 1.6642e-04,\n",
      "         9.5197e-05, 1.0077e-04, 2.7533e-02, 9.7120e-01],\n",
      "        [9.7801e-01, 3.6062e-04, 8.6161e-05, 1.3850e-04, 9.1470e-04, 7.5941e-05,\n",
      "         1.3406e-03, 1.0190e-04, 1.8805e-02, 1.6573e-04]], device='cuda:0')\n",
      "after tensor([[1.9309e-02],\n",
      "        [9.1881e-03],\n",
      "        [6.2283e-03],\n",
      "        [6.2457e-02],\n",
      "        [1.7172e-02],\n",
      "        [1.2551e-01],\n",
      "        [9.0833e-02],\n",
      "        [2.9449e-01],\n",
      "        [1.8298e-02],\n",
      "        [7.8766e-04],\n",
      "        [1.0000e+00],\n",
      "        [8.9772e-04],\n",
      "        [2.3006e-03],\n",
      "        [6.8192e-03],\n",
      "        [1.5648e-02],\n",
      "        [5.6339e-01]], device='cuda:0')\n",
      "reduce ood tensor([[1.3255e-05, 2.6101e-05, 1.8490e-05, 9.2093e-06, 5.1044e-05, 9.9980e-01,\n",
      "         3.9423e-05, 1.1182e-05, 1.0847e-05, 1.8454e-05],\n",
      "        [1.4648e-04, 9.9906e-01, 4.1601e-05, 4.2708e-05, 2.3789e-05, 2.8035e-05,\n",
      "         5.0390e-05, 3.5793e-05, 4.2394e-04, 1.4518e-04],\n",
      "        [5.8657e-05, 9.9737e-01, 7.4861e-05, 3.4004e-05, 4.9588e-05, 9.7604e-05,\n",
      "         1.1858e-04, 7.6186e-05, 1.8514e-03, 2.7409e-04],\n",
      "        [1.2833e-02, 1.7433e-04, 9.8384e-01, 2.9191e-04, 4.3859e-04, 4.5317e-04,\n",
      "         3.8120e-04, 2.1720e-04, 9.4193e-04, 4.2570e-04],\n",
      "        [3.2633e-04, 4.6541e-04, 1.4933e-03, 4.4200e-04, 1.4910e-01, 9.2480e-05,\n",
      "         8.4698e-01, 2.8040e-04, 3.4392e-04, 4.7013e-04],\n",
      "        [1.8168e-04, 3.5109e-04, 2.5772e-04, 7.7766e-01, 1.6540e-04, 4.3291e-04,\n",
      "         2.2010e-01, 5.1173e-04, 1.6635e-04, 1.7112e-04],\n",
      "        [8.4920e-05, 1.1653e-03, 2.9380e-04, 8.6123e-04, 5.2259e-05, 9.9624e-01,\n",
      "         7.3115e-05, 1.3663e-04, 2.2848e-05, 1.0731e-03],\n",
      "        [2.0465e-03, 9.4221e-05, 9.8653e-01, 6.9425e-04, 1.1016e-03, 1.6528e-04,\n",
      "         8.9855e-03, 1.6324e-04, 7.5291e-05, 1.4425e-04],\n",
      "        [3.5331e-05, 4.3999e-05, 4.0182e-05, 4.0354e-03, 1.7606e-04, 9.9548e-01,\n",
      "         6.5953e-05, 4.3240e-05, 5.6924e-05, 2.7738e-05],\n",
      "        [1.9636e-04, 3.7439e-02, 2.3256e-04, 1.9173e-04, 1.7706e-04, 3.6012e-04,\n",
      "         3.6768e-04, 3.3324e-04, 3.7459e-04, 9.6033e-01],\n",
      "        [6.1428e-01, 2.8493e-03, 1.4647e-03, 6.5149e-02, 1.7013e-01, 1.3159e-01,\n",
      "         8.1307e-04, 2.5494e-03, 6.1626e-03, 5.0208e-03],\n",
      "        [8.1545e-05, 1.0018e-04, 4.4003e-05, 8.6916e-05, 4.3501e-05, 3.7770e-05,\n",
      "         4.3499e-05, 8.6863e-05, 1.2901e-03, 9.9819e-01],\n",
      "        [1.2376e-05, 1.5061e-05, 2.2802e-05, 3.1526e-04, 2.8542e-05, 9.9950e-01,\n",
      "         5.2640e-05, 2.3501e-05, 1.2178e-05, 1.7367e-05],\n",
      "        [1.2065e-04, 1.3106e-03, 1.5667e-04, 1.3267e-04, 2.5188e-04, 4.0661e-04,\n",
      "         1.1383e-04, 9.9693e-01, 1.1468e-04, 4.5852e-04],\n",
      "        [4.7687e-04, 6.6445e-04, 3.4115e-04, 3.8331e-04, 6.6805e-04, 5.2671e-04,\n",
      "         2.6949e-04, 2.6138e-04, 4.6368e-01, 5.3273e-01],\n",
      "        [9.6859e-01, 3.4420e-04, 1.3133e-04, 1.0730e-04, 1.4600e-03, 6.6824e-05,\n",
      "         9.9861e-04, 1.3725e-04, 2.7938e-02, 2.2638e-04]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "d_model.eval()\n",
    "\n",
    "model.cuda()\n",
    "d_model.cuda()\n",
    "from attacks import ood_pgd\n",
    "attack = ood_pgd\n",
    "ce_loss = torch.nn.BCELoss()\n",
    "soft = torch.nn.Softmax(dim=1)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, requires_grad = True), Variable(target)\n",
    "\n",
    "        d_target = torch.unsqueeze(torch.ones(16), dim=1).cuda()\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # print(ce_loss(d_model(data), d_target))\n",
    "        print(\"before\", d_model(data))\n",
    "        print(\"before confidence\", soft(outputs.data))\n",
    "        # print(outputs)\n",
    "        # print(d_model(data))\n",
    "        with torch.enable_grad():\n",
    "            attack_inputs = attack.perturb(model, data, data.clone(), target, num_steps=10, attack=True, step_size=0.07, epsilon=0.031, celoss=True)\n",
    "        print(\"after attack\", soft(model(attack_inputs).data))\n",
    "        with torch.enable_grad():\n",
    "            inputs = attack.perturb(d_model, attack_inputs, attack_inputs.clone(), d_target, num_steps=20, attack=True, step_size=0.07, epsilon=0.031, celoss=False)\n",
    "        # with torch.enable_grad():\n",
    "        #     inputs = attack.perturb(model, data, data.clone(), target, num_steps=1, attack=True, epsilon=0.0031)\n",
    "        print(\"after\", d_model(inputs))\n",
    "        print(\"reduce ood\", soft(model(inputs).data))\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# i = 0\n",
    "# for param in d.parameters():\n",
    "#     if i > 109:\n",
    "#         break\n",
    "#     param.requires_grad = False\n",
    "#     # print(param, i)\n",
    "#     i+=1\n",
    "in_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
    "\n",
    "train_loader, test_loader = data_loader.getTargetDataSet('cifar10', 16, in_transform, 'output/resnet_cifar10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: output/resnet_cifar10/svhn-data/train_32x32.mat\n",
      "Using downloaded and verified file: output/resnet_cifar10/svhn-data/test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [00:04, 23.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.490150451660156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "204it [00:08, 23.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.75642776489258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "303it [00:13, 23.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.74287414550781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "405it [00:17, 23.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.498748779296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "504it [00:21, 23.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.951026916503906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "603it [00:25, 23.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.6612434387207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "705it [00:30, 23.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.648902893066406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "804it [00:34, 23.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.61113739013672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "903it [00:38, 23.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.178043365478516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1005it [00:43, 23.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.086097717285156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1104it [00:47, 23.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.34327697753906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1203it [00:51, 23.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.0681037902832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1305it [00:56, 23.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.83881378173828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1404it [01:00, 23.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.71826934814453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1503it [01:04, 23.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.945281982421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1605it [01:08, 23.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.28695297241211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1704it [01:13, 23.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.79509353637695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1803it [01:17, 23.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.45842742919922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1905it [01:21, 23.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.23237609863281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2004it [01:26, 23.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.55979919433594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2103it [01:30, 23.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.59657287597656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2205it [01:34, 23.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.47576904296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2304it [01:39, 23.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.28524398803711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2403it [01:43, 23.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.46026611328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2505it [01:47, 23.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.878273010253906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2604it [01:52, 22.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.0124626159668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2703it [01:56, 23.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.32334518432617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2805it [02:00, 23.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.67756271362305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2904it [02:05, 23.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.98637771606445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3003it [02:09, 23.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.40641403198242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3102it [02:13, 23.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.5000114440918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3125it [02:14, 23.20it/s]\n"
     ]
    }
   ],
   "source": [
    "d_model = AC(model, 10, outf, True, 'resnet', sample_mean, precision, 5, 0.0)\n",
    "\n",
    "learning_rate = 0.001\n",
    "criterion = torch.nn.BCELoss()\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "out_train_loader, out_test_loader = data_loader.getTargetDataSet('svhn', 16, in_transform, 'output/resnet_cifar10/')\n",
    "\n",
    "for epoch in range(1):\n",
    "    i = 0\n",
    "    for data, data2 in tqdm(zip(train_loader, out_train_loader)):\n",
    "        data, labels = data[0].cuda(), data[1].cuda()\n",
    "        data, labels = Variable(data, requires_grad = True), Variable(labels)\n",
    "        labels = torch.ones(labels.size()[0]).cuda()\n",
    "\n",
    "        out_data, out_labels = data2[0].cuda(), data2[1].cuda()\n",
    "        out_data, out_labels = Variable(out_data, requires_grad = True), Variable(out_labels)\n",
    "        out_labels = torch.zeros(out_labels.size()[0]).cuda()\n",
    "        # print(data)\n",
    "        data = torch.concat((data, out_data), 0)\n",
    "        labels = torch.concat((labels,out_labels), 0)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = d_model(data)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        # outputs = outputs.view(outputs.size(0))\n",
    "        # print(outputs.shape)\n",
    "        loss = criterion(outputs, labels.type(torch.float))\n",
    "        # print(loss)\n",
    "        # break\n",
    "        # loss = Variable(loss, requires_grad=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            print(loss.item())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_Mahalanobis_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/zjiaming/deep_Mahalanobis_detector/test_oodreducer.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnioru.eecs.umich.edu/home/zjiaming/deep_Mahalanobis_detector/test_oodreducer.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m M_in \u001b[39m=\u001b[39m get_Mahalanobis_score(model, test_loader, \u001b[39m10\u001b[39m, outf, \\\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnioru.eecs.umich.edu/home/zjiaming/deep_Mahalanobis_detector/test_oodreducer.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                 \u001b[39mTrue\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mresnet\u001b[39m\u001b[39m'\u001b[39m, sample_mean, precision, \u001b[39m0\u001b[39m, \u001b[39m0.0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_Mahalanobis_score' is not defined"
     ]
    }
   ],
   "source": [
    "M_in = get_Mahalanobis_score(model, test_loader, 10, outf, \\\n",
    "                True, 'resnet', sample_mean, precision, 0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Mahalanobis_score(model, test_loader, num_classes, outf, out_flag, net_type, sample_mean, precision, layer_index, magnitude):\n",
    "    '''\n",
    "    Compute the proposed Mahalanobis confidence score on input dataset\n",
    "    return: Mahalanobis score from layer_index\n",
    "    '''\n",
    "    model.eval()\n",
    "    Mahalanobis = torch.tensor([1]).cuda()\n",
    "    # Mahalanobis = []\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, requires_grad = True), Variable(target)\n",
    "        \n",
    "        out_features = model.intermediate_forward(data, layer_index)\n",
    "        \n",
    "        out_features = out_features.view(out_features.size(0), out_features.size(1), -1)\n",
    "        out_features = torch.mean(out_features, 2)\n",
    "\n",
    "        # compute Mahalanobis score\n",
    "        gaussian_score = 0\n",
    "        for i in range(num_classes):\n",
    "            batch_sample_mean = sample_mean[layer_index][i]\n",
    "            # zero_f = out_features.data - batch_sample_mean\n",
    "            zero_f = out_features - batch_sample_mean\n",
    "            term_gau = -0.5*torch.mm(torch.mm(zero_f, precision[layer_index]), zero_f.t()).diag()\n",
    "            if i == 0:\n",
    "                gaussian_score = term_gau.view(-1,1)\n",
    "            else:\n",
    "                gaussian_score = torch.cat((gaussian_score, term_gau.view(-1,1)), 1)\n",
    "        \n",
    "        gaussian_score, _ = torch.max(gaussian_score, dim=1)\n",
    "        Mahalanobis = torch.cat((Mahalanobis, gaussian_score))\n",
    "\n",
    "        \n",
    "        # # Input_processing\n",
    "        # sample_pred = gaussian_score.max(1)[1]\n",
    "        # batch_sample_mean = sample_mean[layer_index].index_select(0, sample_pred)\n",
    "        # zero_f = out_features - Variable(batch_sample_mean)\n",
    "        # pure_gau = -0.5*torch.mm(torch.mm(zero_f, Variable(precision[layer_index])), zero_f.t()).diag()\n",
    "        # loss = torch.mean(-pure_gau)\n",
    "        # loss.backward()\n",
    "         \n",
    "        # gradient =  torch.ge(data.grad.data, 0)\n",
    "        # gradient = (gradient.float() - 0.5) * 2\n",
    "        # if net_type == 'densenet':\n",
    "        #     gradient.index_copy_(1, torch.LongTensor([0]).cuda(), gradient.index_select(1, torch.LongTensor([0]).cuda()) / (63.0/255.0))\n",
    "        #     gradient.index_copy_(1, torch.LongTensor([1]).cuda(), gradient.index_select(1, torch.LongTensor([1]).cuda()) / (62.1/255.0))\n",
    "        #     gradient.index_copy_(1, torch.LongTensor([2]).cuda(), gradient.index_select(1, torch.LongTensor([2]).cuda()) / (66.7/255.0))\n",
    "        # elif net_type == 'resnet':\n",
    "        #     gradient.index_copy_(1, torch.LongTensor([0]).cuda(), gradient.index_select(1, torch.LongTensor([0]).cuda()) / (0.2023))\n",
    "        #     gradient.index_copy_(1, torch.LongTensor([1]).cuda(), gradient.index_select(1, torch.LongTensor([1]).cuda()) / (0.1994))\n",
    "        #     gradient.index_copy_(1, torch.LongTensor([2]).cuda(), gradient.index_select(1, torch.LongTensor([2]).cuda()) / (0.2010))\n",
    "        # tempInputs = torch.add(data.data, -magnitude, gradient)\n",
    " \n",
    "        # noise_out_features = model.intermediate_forward(Variable(tempInputs, volatile=True), layer_index)\n",
    "        # noise_out_features = noise_out_features.view(noise_out_features.size(0), noise_out_features.size(1), -1)\n",
    "        # noise_out_features = torch.mean(noise_out_features, 2)\n",
    "        # noise_gaussian_score = 0\n",
    "        # for i in range(num_classes):\n",
    "        #     batch_sample_mean = sample_mean[layer_index][i]\n",
    "        #     zero_f = noise_out_features.data - batch_sample_mean\n",
    "        #     term_gau = -0.5*torch.mm(torch.mm(zero_f, precision[layer_index]), zero_f.t()).diag()\n",
    "        #     if i == 0:\n",
    "        #         noise_gaussian_score = term_gau.view(-1,1)\n",
    "        #     else:\n",
    "        #         noise_gaussian_score = torch.cat((noise_gaussian_score, term_gau.view(-1,1)), 1)      \n",
    "\n",
    "        # noise_gaussian_score, _ = torch.max(noise_gaussian_score, dim=1)\n",
    "        # # print(noise_gaussian_score)\n",
    "        # Mahalanobis = torch.cat((Mahalanobis, noise_gaussian_score))\n",
    "        # Mahalanobis.extend(noise_gaussian_score.cpu().numpy())\n",
    "\n",
    "    return Mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([17.2188,  9.3410,  1.6312,  ..., -1.2758,  1.6002,  1.8951],\n",
       "        device='cuda:0', grad_fn=<CatBackward0>),)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.empty((10000))[0] = torch.tensor(5)\n",
    "torch.gradient(M_in[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_in = get_Mahalanobis_score(model, test_loader, 10, outf, \\\n",
    "                            True, 'resnet', sample_mean, precision, 0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -7.8309,  -7.8422,  -4.4515,  -4.7944,  -3.8910,  -4.7397,  -3.7094,\n",
       "          -4.7983,  -8.9876,  -7.9356],\n",
       "        [-14.0703, -11.5042, -10.5742,  -9.2915, -11.0764,  -9.4256, -10.4809,\n",
       "          -8.4230, -15.2782, -10.7860],\n",
       "        [ -7.8402, -12.6759, -11.3069, -11.6400, -12.1074, -12.2116, -12.6828,\n",
       "         -12.3974,  -9.7964, -13.1036],\n",
       "        [-11.1188, -13.9470, -13.5376, -15.1099, -14.2422, -15.1046, -15.3131,\n",
       "         -15.6443,  -9.2849, -14.3039],\n",
       "        [ -8.7679,  -8.5532, -13.2916, -14.3393, -14.5887, -15.8007, -14.4166,\n",
       "         -13.8077,  -7.5496,  -8.6151],\n",
       "        [-21.7121, -22.3444, -15.8825, -18.6516, -15.0568, -18.6731, -15.4293,\n",
       "         -17.9079, -23.3436, -22.4998],\n",
       "        [ -9.8352,  -7.9063,  -7.6867,  -6.3545,  -8.2395,  -6.1512,  -7.7033,\n",
       "          -5.9360, -10.9791,  -7.1236],\n",
       "        [ -9.3610, -10.7226, -13.5705, -13.5829, -14.3001, -14.5927, -14.5812,\n",
       "         -13.4367,  -9.2280, -10.2005],\n",
       "        [-17.8524, -17.5579, -17.2364, -14.6830, -16.9262, -14.4168, -15.9918,\n",
       "         -17.8866, -19.0249, -19.1185],\n",
       "        [-12.0135, -11.1169, -10.5349,  -7.8792, -10.7306,  -7.0718, -10.5468,\n",
       "          -8.7138, -13.0960, -10.6694],\n",
       "        [-19.5075, -17.8135, -15.2426, -12.7513, -15.2552, -11.3655, -14.0677,\n",
       "         -13.4265, -20.1925, -16.4407],\n",
       "        [-12.5071, -16.8189, -13.7792, -11.8554, -13.9704, -10.8249, -15.0350,\n",
       "         -14.0214, -13.9385, -15.5489],\n",
       "        [ -8.7176,  -8.0250,  -5.0115,  -3.8709,  -4.8725,  -3.9915,  -4.8640,\n",
       "          -5.8064,  -9.9929,  -7.8481],\n",
       "        [-13.6535, -12.6177,  -9.9113,  -7.7279,  -9.9555,  -7.9775,  -9.3087,\n",
       "          -9.7223, -14.7787, -12.3441],\n",
       "        [-10.1428,  -7.6559,  -7.6246,  -7.5575,  -6.8436,  -6.4225,  -7.1658,\n",
       "          -6.8554, -10.0621,  -7.9124],\n",
       "        [ -7.9359,  -7.0425,  -5.6080,  -6.3543,  -5.3663,  -6.4557,  -5.6227,\n",
       "          -4.5274,  -7.4775,  -5.2264]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_in    \n",
    "# len(M_in)\n",
    "# model = LogisticRegression(5, 1)\n",
    "# torch.squeeze(model(X_train)).round().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3f42186ae79af8aae2009356c36c131fa2dc7d89e01a3aaa011813e625649e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
